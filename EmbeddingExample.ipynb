{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2Ld16NmEq0Eyuh00UMJa8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajAakash/Transformers/blob/main/EmbeddingExample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1oyg2Aze4Jt",
        "outputId": "621708ba-38e2-4038-9003-375dd0d2559f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here is X_train   (0, 0)\t1\n",
            "  (0, 2)\t1\n",
            "  (1, 2)\t1\n",
            "  (1, 3)\t1\n",
            "  (2, 0)\t1\n",
            "  (2, 1)\t1\n",
            "  (2, 2)\t1\n",
            "row-  (0, 0)\t1\n",
            "  (0, 2)\t1-row here\n",
            "Parameter containing:\n",
            "tensor([[ 0.3552,  0.3508,  0.4610, -0.4364],\n",
            "        [ 0.4581,  0.0826,  0.0405,  0.2970],\n",
            "        [-0.1293,  0.2985,  0.1227, -0.3119]], requires_grad=True)\n",
            "tensor([[ 0.8162,  0.4986, -0.0067]], grad_fn=<MmBackward0>)\n",
            "Parameter containing:\n",
            "tensor([[ 0.3552,  0.4581, -0.1293],\n",
            "        [ 0.3508,  0.0826,  0.2985],\n",
            "        [ 0.4610,  0.0405,  0.1227],\n",
            "        [-0.4364,  0.2970, -0.3119]])\n",
            "tensor([0.4610, 0.0405, 0.1227])\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# Create a sparse matrix X_train\n",
        "X_train = csr_matrix(np.array([[1, 0, 1, 0],\n",
        "                               [0, 0, 1, 1],\n",
        "                               [1, 1, 1, 0]]))\n",
        "\n",
        "print(f'here is X_train {X_train}')\n",
        "\n",
        "# Get the first row of the sparse matrix\n",
        "row = X_train.getrow(0)\n",
        "\n",
        "print(f'row-{row}-row here')\n",
        "\n",
        "# Create a linear layer with input size 4 and output size 3 (bias is set to False)\n",
        "w_linear = nn.Linear(4, 3, bias=False)\n",
        "\n",
        "# Print the weight of the linear layer\n",
        "print(w_linear.weight)\n",
        "\n",
        "# Print the output of the linear layer when applied to the first row of the sparse matrix\n",
        "print(w_linear(torch.FloatTensor(row.toarray())))\n",
        "\n",
        "# Create an embedding layer with input size 4 and output size 3, initialize with the weights from the linear layer\n",
        "w_embedding = nn.Embedding(4, 3).from_pretrained(w_linear.weight.T)\n",
        "\n",
        "# Print the weight of the embedding layer\n",
        "print(w_embedding.weight)\n",
        "\n",
        "# Print the output of the embedding layer when applied to the sum of indices of the first row\n",
        "print(w_embedding(torch.tensor(row.indices).sum(0)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define an embedding layer with vocabulary size 100 and embedding dimension 32\n",
        "embedding_layer = nn.Embedding(100, 32)\n",
        "\n",
        "# Input tensor with integer indices\n",
        "input_indices = torch.tensor([1, 5, 20, 3])\n",
        "\n",
        "# Apply the embedding layer\n",
        "output = embedding_layer(input_indices)\n",
        "\n",
        "print(output.shape)  # Output shape: torch.Size([4, 32]) - 4 rows (input indices), each with a 32-dimensional embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbdQudJQtBRW",
        "outputId": "30e282fd-f34d-4258-c9d4-09475c14f1dd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a linear layer with input size 10 and output size 5\n",
        "linear_layer = nn.Linear(10, 5)\n",
        "\n",
        "# Input tensor with size (batch_size, input_size)\n",
        "input_data = torch.rand(32, 10)\n",
        "\n",
        "# Apply the linear layer\n",
        "output = linear_layer(input_data)\n",
        "\n",
        "print(output.shape)  # Output shape: torch.Size([32, 5]) - 32 rows (batch size), each with 5 output values\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dav9_4u5uMbL",
        "outputId": "1672a2c4-adbe-4b33-d752-f0016644a82e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xibRC0tguXGa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}